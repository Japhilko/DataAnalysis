install.packages("SparkR")
library("SparkR")
library("SparkR")
getwd()
setwd("C:/Users/kolbjp/Documents/GitHub/DataAnalysis")
setwd("UseCases/")
load("data/GermanCities_Wikipedia_dtm.RData")
dtm2 <- as.matrix(dtm)
frequency <- colSums(dtm2)
dtm
dtm2 <- as.matrix(dtm)
frequency <- colSums(dtm2)
frequency <- sort(frequency, decreasing=TRUE)
load("data/GermanCities_Wikipedia.RData")
docs <- Corpus(VectorSource(articles))
library("stringi")
library("tm")
library("FactoMineR")
library(maps)
docs <- Corpus(VectorSource(articles))
docs2 <- tm_map(docs, function(x) stri_replace_all_regex(x, "<.+?>", " "))
docs3 <- tm_map(docs2, function(x) stri_replace_all_fixed(x, "\t", " "))
docs4 <- tm_map(docs3, PlainTextDocument)
docs5 <- tm_map(docs4, stripWhitespace)
docs6 <- tm_map(docs5, removeWords, stopwords("english"))
docs7 <- tm_map(docs6, removePunctuation)
docs8 <- tm_map(docs7, tolower)
docs8 <- tm_map(docs8, PlainTextDocument)
dtm <- DocumentTermMatrix(docs8)
dtm2 <- as.matrix(dtm)
frequency <- colSums(dtm2)
frequency <- sort(frequency, decreasing=TRUE)
words <- frequency[frequency>20]
s <- dtm2[1,which(colnames(dtm2) %in% names(words))]
for(i in 2:nrow(dtm2)){
s <- cbind(s,dtm2[i,which(colnames(dtm2) %in% names(words))])
}
colnames(s) <- titles
wiki <- "http://en.wikipedia.org/wiki/"
titles <- dcitiesb$name
titles <- gsub(" ","_",titles)
kable(head(world.cities))
dcities <- world.cities[world.cities$country.etc=="Germany",]
kable(head(dcities))
dcities[which.min(dcities$pop),]
dcitiesb <- dcities[dcities$pop>100000,]
wiki <- "http://en.wikipedia.org/wiki/"
titles <- dcitiesb$name
titles <- gsub(" ","_",titles)
colnames(s) <- titles
PCA(s)
res_pca <- PCA(s)
summary(res_pca)
head(s)
dtm2
words
words[1]
words[2]
words[3]
words[4]
words[5]
words6
words[6]
words[61]
words[60]
names(words)
res_pca
summary(res_pca)
s
head(s)
rownames(s) <- words
res_pca <- PCA(s)
res_pca
summary(res_pca)
s
head(s)
rownames(s)
rownames(s) <- names(words)
head(s)
res_pca <- PCA(s)
res_pca
summary(res_pca)
